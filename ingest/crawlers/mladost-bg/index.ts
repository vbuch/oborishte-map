#!/usr/bin/env node

import dotenv from "dotenv";
import { resolve } from "node:path";
import { Browser } from "playwright";
import type { Firestore } from "firebase-admin/firestore";
import { SourceDocument, PostLink } from "./types";
import { launchBrowser } from "../shared/browser";
import { delay } from "@/lib/delay";
import { isUrlProcessed, saveSourceDocument } from "../shared/firestore";
import { extractPostLinks, extractPostDetails } from "./extractors";
import { buildWebPageSourceDocument } from "../shared/webpage-crawlers";
import { parseShortBulgarianDateTime } from "../shared/date-utils";

// Load environment variables from .env.local
dotenv.config({ path: resolve(process.cwd(), ".env.local") });

const INDEX_URL =
  "https://mladost.bg/%d0%b2%d1%81%d0%b8%d1%87%d0%ba%d0%b8-%d0%bd%d0%be%d0%b2%d0%b8%d0%bd%d0%b8/%d0%b8%d0%bd%d1%84%d0%be%d1%80%d0%bc%d0%b0%d1%86%d0%b8%d1%8f-%d0%be%d1%82%d0%bd%d0%be%d1%81%d0%bd%d0%be-%d0%bf%d0%bb%d0%b0%d0%bd%d0%be%d0%b2%d0%b8%d1%82%d0%b5-%d1%80%d0%b5%d0%bc%d0%be%d0%bd%d1%82/";
const SOURCE_TYPE = "mladost-bg";
const DELAY_BETWEEN_REQUESTS = 2000; // 2 seconds

/**
 * Process a single post
 */
async function processPost(
  browser: Browser,
  postLink: PostLink,
  adminDb: Firestore
): Promise<void> {
  const { url, title, date, time } = postLink;

  console.log(`\nüîç Processing: ${title.substring(0, 60)}...`);

  // Check if already processed
  try {
    const alreadyProcessed = await isUrlProcessed(url, adminDb);
    if (alreadyProcessed) {
      console.log(`‚è≠Ô∏è  Skipped (already processed): ${url}`);
      return;
    }
  } catch (error) {
    console.error(`‚ùå Error checking if URL is processed: ${url}`, error);
    throw error;
  }

  // Open new page for this post
  const page = await browser.newPage();

  try {
    console.log(`üì• Fetching: ${url}`);
    await page.goto(url, { waitUntil: "networkidle" });

    // Extract post details
    const details = await extractPostDetails(page);

    // Combine date and time from index page for custom parser
    const dateText = time ? `${date} ${time}` : date;

    // Use buildWebPageSourceDocument with custom date parser for DD.MM.YY format
    const postDetails = buildWebPageSourceDocument(
      url,
      details.title || title, // Prefer detail page title, fallback to index
      dateText,
      details.contentHtml,
      SOURCE_TYPE,
      (dateStr) => {
        const [datePart, timePart] = dateStr.split(" ");
        return parseShortBulgarianDateTime(datePart, timePart);
      }
    ) as Omit<SourceDocument, "crawledAt">;

    // Save to Firestore
    const sourceDoc: SourceDocument = {
      ...postDetails,
      crawledAt: new Date(),
    };

    await saveSourceDocument(sourceDoc, adminDb);

    console.log(`‚úÖ Successfully processed: ${title.substring(0, 60)}...`);
  } catch (error) {
    console.error(`‚ùå Error processing post: ${url}`, error);
    throw error; // Re-throw to fail the entire process
  } finally {
    await page.close();
  }

  // Wait before next request
  await delay(DELAY_BETWEEN_REQUESTS);
}

/**
 * Main crawler function
 */
export async function crawl(): Promise<void> {
  console.log("üöÄ Starting mladost-bg crawler...\n");
  console.log(`üìç Index URL: ${INDEX_URL}`);
  console.log(`üóÑÔ∏è  Source type: ${SOURCE_TYPE}\n`);

  // Import firebase-admin after env is loaded
  const { adminDb } = await import("@/lib/firebase-admin");

  let browser: Browser | null = null;

  try {
    // Launch browser
    console.log("üåê Launching browser...");
    browser = await launchBrowser();

    // Open index page
    const page = await browser.newPage();
    console.log(`üì• Fetching index page: ${INDEX_URL}`);
    await page.goto(INDEX_URL, { waitUntil: "networkidle" });

    // Extract all post links
    const postLinks = await extractPostLinks(page);
    await page.close();

    if (postLinks.length === 0) {
      console.warn("‚ö†Ô∏è No posts found on index page");
      return;
    }

    console.log(`\nüìä Total posts to process: ${postLinks.length}\n`);

    // Process each post
    let processedCount = 0;
    let skippedCount = 0;

    for (const postLink of postLinks) {
      try {
        const wasProcessed = await isUrlProcessed(postLink.url, adminDb);

        if (wasProcessed) {
          skippedCount++;
        } else {
          await processPost(browser, postLink, adminDb);
          processedCount++;
        }
      } catch (error) {
        console.error(`‚ùå Error processing post: ${postLink.url}`, error);
        // Continue with next post
      }
    }

    console.log("\n" + "=".repeat(60));
    console.log("‚úÖ Crawling completed successfully!");
    console.log(`üìä Total posts found: ${postLinks.length}`);
    console.log(`‚úÖ Newly processed: ${processedCount}`);
    console.log(`‚è≠Ô∏è  Skipped (already exists): ${skippedCount}`);
    console.log("=".repeat(60) + "\n");
  } catch (error) {
    console.error("\n" + "=".repeat(60));
    console.error("‚ùå Crawling failed with error:");
    console.error(error);
    console.error("=".repeat(60) + "\n");
    throw error;
  } finally {
    if (browser) {
      await browser.close();
      console.log("üîí Browser closed");
    }
  }
}

// Run the crawler if executed directly
if (require.main === module) {
  // eslint-disable-next-line unicorn/prefer-top-level-await
  crawl().catch((error) => {
    console.error("Fatal error:", error);
    process.exit(1);
  });
}
